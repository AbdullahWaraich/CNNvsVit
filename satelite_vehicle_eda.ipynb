{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi6h0eEbx7kb"
      },
      "source": [
        "# Exploratory Data Analysis - Satellite Vehicle Detection Dataset\n",
        "\n",
        "**Research Project**: Comparative Analysis of CNN (YOLOv11) vs Vision Transformer (RF-DETR)\n",
        "\n",
        "**Author**: Abdullah Waraich\n",
        "\n",
        "## Overview\n",
        "This notebook performs exploratory data analysis on the satellite vehicle detection dataset from Roboflow. The dataset contains satellite imagery with annotated vehicles and will be used to compare CNN vs Transformer approaches for object detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv4Ccnjjx7kd"
      },
      "source": [
        "## 1. Setup and Data Loading\n",
        "\n",
        "First, we will install  required packages and download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQK03quRx7kd"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install roboflow matplotlib seaborn pandas numpy opencv-python pillow plotly\n",
        "\n",
        "# Import essential libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from collections import Counter\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHTuh2Lyx7ke"
      },
      "outputs": [],
      "source": [
        "# Download dataset from Roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"--------------\") # Replace it with your own key OR get code snippet directly from RoboFlow for downloading this dataset\n",
        "project = rf.workspace(\"ab-ml-cv\").project(\"satalite-blffa\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov11\")\n",
        "\n",
        "print(f\" Dataset downloaded to: {dataset.location}\")\n",
        "\n",
        "# Define paths\n",
        "dataset_path = dataset.location\n",
        "train_images_path = os.path.join(dataset_path, \"train\", \"images\")\n",
        "train_labels_path = os.path.join(dataset_path, \"train\", \"labels\")\n",
        "val_images_path = os.path.join(dataset_path, \"valid\", \"images\")\n",
        "val_labels_path = os.path.join(dataset_path, \"valid\", \"labels\")\n",
        "test_images_path = os.path.join(dataset_path, \"test\", \"images\")\n",
        "test_labels_path = os.path.join(dataset_path, \"test\", \"labels\")\n",
        "\n",
        "print(\"\\n Dataset structure:\")\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    level = root.replace(dataset_path, '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files[:3]:  # Show first 3 files\n",
        "        print(f\"{subindent}{file}\")\n",
        "    if len(files) > 3:\n",
        "        print(f\"{subindent}... and {len(files)-3} more files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnQF8GU3x7ke"
      },
      "source": [
        "## 2. Dataset Overview and Basic Statistics\n",
        "\n",
        "We will start start by understanding the basic structure of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN_c8PCQx7ke"
      },
      "outputs": [],
      "source": [
        "# Load class names\n",
        "with open(os.path.join(dataset_path, \"data.yaml\"), 'r') as f:\n",
        "    import yaml\n",
        "    data_config = yaml.safe_load(f)\n",
        "\n",
        "class_names = data_config['names']\n",
        "num_classes = data_config['nc']\n",
        "\n",
        "print(\" CLASS INFORMATION\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Class names: {class_names}\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Count files in each split\n",
        "def count_files(path):\n",
        "    return len([f for f in os.listdir(path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
        "\n",
        "train_count = count_files(train_images_path)\n",
        "val_count = count_files(val_images_path)\n",
        "test_count = count_files(test_images_path)\n",
        "total_images = train_count + val_count + test_count\n",
        "\n",
        "print(\"\\n DATASET SPLIT STATISTICS\")\n",
        "print(f\"Training images:   {train_count:,} ({train_count/total_images:.1%})\")\n",
        "print(f\"Validation images: {val_count:,} ({val_count/total_images:.1%})\")\n",
        "print(f\"Test images:       {test_count:,} ({test_count/total_images:.1%})\")\n",
        "print(f\"Total images:      {total_images:,}\")\n",
        "\n",
        "\n",
        "# Create a summary dictionary for later use\n",
        "dataset_summary = {\n",
        "    'train_images': train_count,\n",
        "    'val_images': val_count,\n",
        "    'test_images': test_count,\n",
        "    'total_images': total_images,\n",
        "    'num_classes': num_classes,\n",
        "    'class_names': class_names\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8IZeBS5x7kf"
      },
      "source": [
        "## 3. Image Analysis\n",
        "\n",
        "Now let's analyze the characteristics of our satellite images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jywkmtm2x7kf"
      },
      "outputs": [],
      "source": [
        "def analyze_images(images_path, sample_size=100):\n",
        "\n",
        "    image_files = [f for f in os.listdir(images_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    # Sample images if dataset is large\n",
        "    if len(image_files) > sample_size:\n",
        "        image_files = np.random.choice(image_files, sample_size, replace=False)\n",
        "\n",
        "    widths, heights, file_sizes, channels = [], [], [], []\n",
        "\n",
        "    print(f\" Analyzing {len(image_files)} images...\")\n",
        "\n",
        "    for i, img_file in enumerate(image_files):\n",
        "        if i % 20 == 0:\n",
        "            print(f\"  Progress: {i+1}/{len(image_files)}\")\n",
        "\n",
        "        img_path = os.path.join(images_path, img_file)\n",
        "\n",
        "        # Get file size\n",
        "        file_size = os.path.getsize(img_path) / 1024  # KB\n",
        "        file_sizes.append(file_size)\n",
        "\n",
        "        # Get image dimensions using PIL (faster than OpenCV)\n",
        "        with Image.open(img_path) as img:\n",
        "            width, height = img.size\n",
        "            widths.append(width)\n",
        "            heights.append(height)\n",
        "\n",
        "            # Get number of channels\n",
        "            if img.mode == 'RGB':\n",
        "                channels.append(3)\n",
        "            elif img.mode == 'RGBA':\n",
        "                channels.append(4)\n",
        "            elif img.mode == 'L':\n",
        "                channels.append(1)\n",
        "            else:\n",
        "                channels.append(len(img.getbands()))\n",
        "\n",
        "    return {\n",
        "        'widths': widths,\n",
        "        'heights': heights,\n",
        "        'file_sizes': file_sizes,\n",
        "        'channels': channels,\n",
        "        'total_analyzed': len(image_files)\n",
        "    }\n",
        "\n",
        "# Analyze training images\n",
        "print(\" ANALYZING TRAINING IMAGES\")\n",
        "train_img_stats = analyze_images(train_images_path, sample_size=200)\n",
        "\n",
        "print(\"\\n IMAGE CHARACTERISTICS\")\n",
        "print(f\"Images analyzed: {train_img_stats['total_analyzed']}\")\n",
        "print(f\"Width range: {min(train_img_stats['widths'])} - {max(train_img_stats['widths'])} pixels\")\n",
        "print(f\"Height range: {min(train_img_stats['heights'])} - {max(train_img_stats['heights'])} pixels\")\n",
        "print(f\"Average dimensions: {np.mean(train_img_stats['widths']):.0f} x {np.mean(train_img_stats['heights']):.0f}\")\n",
        "print(f\"File size range: {min(train_img_stats['file_sizes']):.1f} - {max(train_img_stats['file_sizes']):.1f} KB\")\n",
        "print(f\"Average file size: {np.mean(train_img_stats['file_sizes']):.1f} KB\")\n",
        "print(f\"Color channels: {Counter(train_img_stats['channels'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpM6iAHEx7kf"
      },
      "outputs": [],
      "source": [
        "# Visualize image statistics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Image Characteristics Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Width distribution\n",
        "axes[0, 0].hist(train_img_stats['widths'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0, 0].axvline(np.mean(train_img_stats['widths']), color='red', linestyle='--', label=f\"Mean: {np.mean(train_img_stats['widths']):.0f}\")\n",
        "axes[0, 0].set_title('Image Width Distribution')\n",
        "axes[0, 0].set_xlabel('Width (pixels)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Height distribution\n",
        "axes[0, 1].hist(train_img_stats['heights'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "axes[0, 1].axvline(np.mean(train_img_stats['heights']), color='red', linestyle='--', label=f\"Mean: {np.mean(train_img_stats['heights']):.0f}\")\n",
        "axes[0, 1].set_title('Image Height Distribution')\n",
        "axes[0, 1].set_xlabel('Height (pixels)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Aspect ratio\n",
        "aspect_ratios = [w/h for w, h in zip(train_img_stats['widths'], train_img_stats['heights'])]\n",
        "axes[1, 0].hist(aspect_ratios, bins=30, alpha=0.7, color='gold', edgecolor='black')\n",
        "axes[1, 0].axvline(np.mean(aspect_ratios), color='red', linestyle='--', label=f\"Mean: {np.mean(aspect_ratios):.2f}\")\n",
        "axes[1, 0].set_title('Aspect Ratio Distribution')\n",
        "axes[1, 0].set_xlabel('Width/Height Ratio')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# File size distribution\n",
        "axes[1, 1].hist(train_img_stats['file_sizes'], bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "axes[1, 1].axvline(np.mean(train_img_stats['file_sizes']), color='red', linestyle='--', label=f\"Mean: {np.mean(train_img_stats['file_sizes']):.1f} KB\")\n",
        "axes[1, 1].set_title('File Size Distribution')\n",
        "axes[1, 1].set_xlabel('File Size (KB)')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Key insights\n",
        "print(\"\\nüîç KEY INSIGHTS:\")\n",
        "unique_widths = len(set(train_img_stats['widths']))\n",
        "unique_heights = len(set(train_img_stats['heights']))\n",
        "\n",
        "if unique_widths == 1 and unique_heights == 1:\n",
        "    print(\" All images have consistent dimensions - good for training!\")\n",
        "else:\n",
        "    print(f\"  Images have varying dimensions - {unique_widths} unique widths, {unique_heights} unique heights\")\n",
        "\n",
        "avg_aspect_ratio = np.mean(aspect_ratios)\n",
        "if 0.95 <= avg_aspect_ratio <= 1.05:\n",
        "    print(\" Images are approximately square\")\n",
        "else:\n",
        "    print(f\" Images have aspect ratio of {avg_aspect_ratio:.2f} (width/height)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csEoI7pUx7kf"
      },
      "source": [
        "## 4. Annotation Analysis\n",
        "\n",
        "Now let's dive into the annotations to understand our labeled data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsqbZ6Vqx7kf"
      },
      "outputs": [],
      "source": [
        "def parse_yolo_annotations(labels_path, images_path):\n",
        "\n",
        "    annotations = []\n",
        "    label_files = [f for f in os.listdir(labels_path) if f.endswith('.txt')]\n",
        "\n",
        "    print(f\" Getting {len(label_files)} annotation files...\")\n",
        "\n",
        "    for i, label_file in enumerate(label_files):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"  Progress: {i+1}/{len(label_files)}\")\n",
        "\n",
        "        label_path = os.path.join(labels_path, label_file)\n",
        "\n",
        "        # Get corresponding image dimensions\n",
        "        img_file = label_file.replace('.txt', '.jpg')\n",
        "        img_path = os.path.join(images_path, img_file)\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            img_file = label_file.replace('.txt', '.png')\n",
        "            img_path = os.path.join(images_path, img_file)\n",
        "\n",
        "        if os.path.exists(img_path):\n",
        "            with Image.open(img_path) as img:\n",
        "                img_width, img_height = img.size\n",
        "        else:\n",
        "            print(f\"Could not find image for {label_file}\")\n",
        "            continue\n",
        "\n",
        "        # Parse annotations\n",
        "        with open(label_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            if line.strip():\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 5:\n",
        "                    class_id = int(parts[0])\n",
        "                    x_center = float(parts[1])\n",
        "                    y_center = float(parts[2])\n",
        "                    width = float(parts[3])\n",
        "                    height = float(parts[4])\n",
        "\n",
        "                    # Convert to absolute coordinates for analysis\n",
        "                    abs_width = width * img_width\n",
        "                    abs_height = height * img_height\n",
        "                    abs_area = abs_width * abs_height\n",
        "\n",
        "                    annotations.append({\n",
        "                        'image_file': img_file,\n",
        "                        'class_id': class_id,\n",
        "                        'class_name': class_names[class_id],\n",
        "                        'x_center_norm': x_center,\n",
        "                        'y_center_norm': y_center,\n",
        "                        'width_norm': width,\n",
        "                        'height_norm': height,\n",
        "                        'width_abs': abs_width,\n",
        "                        'height_abs': abs_height,\n",
        "                        'area_abs': abs_area,\n",
        "                        'img_width': img_width,\n",
        "                        'img_height': img_height\n",
        "                    })\n",
        "\n",
        "    return annotations\n",
        "\n",
        "# Parse training annotations\n",
        "print(\" ANALYZING TRAINING ANNOTATIONS\")\n",
        "train_annotations = parse_yolo_annotations(train_labels_path, train_images_path)\n",
        "train_df = pd.DataFrame(train_annotations)\n",
        "\n",
        "print(f\"\\n ANNOTATION STATISTICS\")\n",
        "print(f\"Total annotations: {len(train_df):,}\")\n",
        "print(f\"Images with annotations: {train_df['image_file'].nunique():,}\")\n",
        "print(f\"Average annotations per image: {len(train_df) / train_df['image_file'].nunique():.2f}\")\n",
        "\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\n Sample annotations:\")\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfoX1p7dx7kg"
      },
      "source": [
        "## 5. Class Distribution Analysis\n",
        "\n",
        "Understanding the class balance is crucial for model training. Initially we had 3 classes that were Bus, Car and Truck. Owing to a major imbalance in classes, all classes were grouped into one class i.e. Vehicle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Merging"
      ],
      "metadata": {
        "id": "u1sJQvZMl2pM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2659c968"
      },
      "source": [
        "import os\n",
        "import yaml\n",
        "import shutil\n",
        "\n",
        "# Define the new class name and ID\n",
        "new_class_name = 'vehicle'\n",
        "new_class_id = 0\n",
        "\n",
        "# Define the paths\n",
        "dataset_path = dataset.location # Reuse the dataset.location variable\n",
        "data_yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
        "\n",
        "# Load the current data.yaml\n",
        "with open(data_yaml_path, 'r') as f:\n",
        "    data_config = yaml.safe_load(f)\n",
        "\n",
        "# Update class names and number of classes in data.yaml\n",
        "data_config['names'] = [new_class_name]\n",
        "data_config['nc'] = 1\n",
        "\n",
        "# Save the updated data.yaml\n",
        "with open(data_yaml_path, 'w') as f:\n",
        "    yaml.dump(data_config, f, default_flow_style=False)\n",
        "\n",
        "print(f\"‚úÖ Updated {data_yaml_path} with the new class: {new_class_name} (ID: {new_class_id})\")\n",
        "\n",
        "# Function to update annotation files in a directory\n",
        "def update_annotations(labels_path):\n",
        "    label_files = [f for f in os.listdir(labels_path) if f.endswith('.txt')]\n",
        "    print(f\"\\nUpdating annotations in: {labels_path}\")\n",
        "    for i, label_file in enumerate(label_files):\n",
        "        if i % 500 == 0:\n",
        "            print(f\"  Progress: {i+1}/{len(label_files)}\")\n",
        "        label_path = os.path.join(labels_path, label_file)\n",
        "        updated_lines = []\n",
        "        with open(label_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        for line in lines:\n",
        "            if line.strip():\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 5:\n",
        "                    # All original class IDs (0, 1, 2) will become the new class ID (0)\n",
        "                    updated_line = f\"{new_class_id} {parts[1]} {parts[2]} {parts[3]} {parts[4]}\\n\"\n",
        "                    updated_lines.append(updated_line)\n",
        "        # Overwrite the original file with updated annotations\n",
        "        with open(label_path, 'w') as f:\n",
        "            f.writelines(updated_lines)\n",
        "\n",
        "# Update annotations in train, valid, and test sets\n",
        "update_annotations(train_labels_path) # Reuse the train_labels_path variable\n",
        "update_annotations(val_labels_path) # Reuse the val_labels_path variable\n",
        "update_annotations(test_labels_path) # Reuse the test_labels_path variable\n",
        "\n",
        "print(\"\\n‚úÖ Annotation files updated successfully!\")\n",
        "print(\"\\nDataset classes have been merged into a single 'vehicle' class.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZnoqyG2x7kg"
      },
      "outputs": [],
      "source": [
        "# Class distribution analysis\n",
        "class_counts = train_df['class_name'].value_counts()\n",
        "total_objects = len(train_df)\n",
        "\n",
        "print(\" CLASS DISTRIBUTION\")\n",
        "\n",
        "for class_name, count in class_counts.items():\n",
        "    percentage = (count / total_objects) * 100\n",
        "    print(f\"{class_name:10}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "\n",
        "# Visualize class distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "fig.suptitle('Class Distribution Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Bar plot\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "bars = axes[0].bar(class_counts.index, class_counts.values, color=colors, alpha=0.8, edgecolor='black')\n",
        "axes[0].set_title('Objects per Class')\n",
        "axes[0].set_xlabel('Class')\n",
        "axes[0].set_ylabel('Number of Objects')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, count in zip(bars, class_counts.values):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(class_counts.values)*0.01,\n",
        "                f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "wedges, texts, autotexts = axes[1].pie(class_counts.values, labels=class_counts.index,\n",
        "                                      autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "axes[1].set_title('Class Distribution (%)')\n",
        "\n",
        "# Make percentage text bold\n",
        "for autotext in autotexts:\n",
        "    autotext.set_fontweight('bold')\n",
        "    autotext.set_color('white')\n",
        "\n",
        "# Objects per image by class\n",
        "objects_per_image = train_df.groupby(['image_file', 'class_name']).size().unstack(fill_value=0)\n",
        "mean_objects_per_image = objects_per_image.mean()\n",
        "\n",
        "bars = axes[2].bar(mean_objects_per_image.index, mean_objects_per_image.values,\n",
        "                  color=colors, alpha=0.8, edgecolor='black')\n",
        "axes[2].set_title('Average Objects per Image')\n",
        "axes[2].set_xlabel('Class')\n",
        "axes[2].set_ylabel('Average Count per Image')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for bar, count in zip(bars, mean_objects_per_image.values):\n",
        "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(mean_objects_per_image.values)*0.01,\n",
        "                f'{count:.1f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Class imbalance analysis\n",
        "max_count = class_counts.max()\n",
        "min_count = class_counts.min()\n",
        "imbalance_ratio = max_count / min_count\n",
        "\n",
        "print(f\"\\n CLASS IMBALANCE ANALYSIS\")\n",
        "print(f\"Most frequent class: {class_counts.index[0]} ({class_counts.iloc[0]:,} objects)\")\n",
        "print(f\"Least frequent class: {class_counts.index[-1]} ({class_counts.iloc[-1]:,} objects)\")\n",
        "print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "if imbalance_ratio > 3:\n",
        "    print(\"  Significant class imbalance detected\")\n",
        "elif imbalance_ratio > 2:\n",
        "    print(\"  Moderate class imbalance\")\n",
        "else:\n",
        "    print(\" Relatively balanced classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6fyhnylx7kg"
      },
      "source": [
        "## 6. Object Size Analysis\n",
        "\n",
        "Understanding the object sizes is crucial for satellite imagery, as vehicles appear very small in images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prq5rZYzx7kg"
      },
      "outputs": [],
      "source": [
        "# Object size analysis\n",
        "print(\" OBJECT SIZE ANALYSIS\")\n",
        "\n",
        "\n",
        "# Overall size statistics\n",
        "print(\"Overall Size Statistics:\")\n",
        "print(f\"Width range: {train_df['width_abs'].min():.1f} - {train_df['width_abs'].max():.1f} pixels\")\n",
        "print(f\"Height range: {train_df['height_abs'].min():.1f} - {train_df['height_abs'].max():.1f} pixels\")\n",
        "print(f\"Area range: {train_df['area_abs'].min():.0f} - {train_df['area_abs'].max():.0f} pixels¬≤\")\n",
        "print(f\"Average width: {train_df['width_abs'].mean():.1f} pixels\")\n",
        "print(f\"Average height: {train_df['height_abs'].mean():.1f} pixels\")\n",
        "print(f\"Average area: {train_df['area_abs'].mean():.0f} pixels¬≤\")\n",
        "\n",
        "# Size statistics by class\n",
        "print(\"\\n Size Statistics by Class:\")\n",
        "for class_name in class_names:\n",
        "    class_data = train_df[train_df['class_name'] == class_name]\n",
        "    if len(class_data) > 0:\n",
        "        print(f\"\\n{class_name.upper()}:\")\n",
        "        print(f\"  Count: {len(class_data):,}\")\n",
        "        print(f\"  Avg width: {class_data['width_abs'].mean():.1f} ¬± {class_data['width_abs'].std():.1f} px\")\n",
        "        print(f\"  Avg height: {class_data['height_abs'].mean():.1f} ¬± {class_data['height_abs'].std():.1f} px\")\n",
        "        print(f\"  Avg area: {class_data['area_abs'].mean():.0f} ¬± {class_data['area_abs'].std():.0f} px¬≤\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy5ZjZqzx7kg"
      },
      "outputs": [],
      "source": [
        "# Visualize object sizes\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Object Size Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Width distribution by class\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_data = train_df[train_df['class_name'] == class_name]\n",
        "    if len(class_data) > 0:\n",
        "        axes[0, 0].hist(class_data['width_abs'], alpha=0.6, label=class_name, bins=30)\n",
        "\n",
        "axes[0, 0].set_title('Width Distribution by Class')\n",
        "axes[0, 0].set_xlabel('Width (pixels)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Height distribution by class\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_data = train_df[train_df['class_name'] == class_name]\n",
        "    if len(class_data) > 0:\n",
        "        axes[0, 1].hist(class_data['height_abs'], alpha=0.6, label=class_name, bins=30)\n",
        "\n",
        "axes[0, 1].set_title('Height Distribution by Class')\n",
        "axes[0, 1].set_xlabel('Height (pixels)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Area distribution by class\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_data = train_df[train_df['class_name'] == class_name]\n",
        "    if len(class_data) > 0:\n",
        "        axes[0, 2].hist(class_data['area_abs'], alpha=0.6, label=class_name, bins=30)\n",
        "\n",
        "axes[0, 2].set_title('Area Distribution by Class')\n",
        "axes[0, 2].set_xlabel('Area (pixels¬≤)')\n",
        "axes[0, 2].set_ylabel('Frequency')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Box plots for better comparison\n",
        "sns.boxplot(data=train_df, x='class_name', y='width_abs', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Width Distribution (Box Plot)')\n",
        "axes[1, 0].set_xlabel('Class')\n",
        "axes[1, 0].set_ylabel('Width (pixels)')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "sns.boxplot(data=train_df, x='class_name', y='height_abs', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Height Distribution (Box Plot)')\n",
        "axes[1, 1].set_xlabel('Class')\n",
        "axes[1, 1].set_ylabel('Height (pixels)')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "sns.boxplot(data=train_df, x='class_name', y='area_abs', ax=axes[1, 2])\n",
        "axes[1, 2].set_title('Area Distribution (Box Plot)')\n",
        "axes[1, 2].set_xlabel('Class')\n",
        "axes[1, 2].set_ylabel('Area (pixels¬≤)')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Small object analysis\n",
        "print(\"\\n SMALL OBJECT ANALYSIS\")\n",
        "\n",
        "\n",
        "# Define small object thresholds (common in object detection)\n",
        "small_area_threshold = 32 * 32  # 32x32 pixels\n",
        "medium_area_threshold = 96 * 96  # 96x96 pixels\n",
        "\n",
        "small_objects = train_df[train_df['area_abs'] <= small_area_threshold]\n",
        "medium_objects = train_df[(train_df['area_abs'] > small_area_threshold) &\n",
        "                         (train_df['area_abs'] <= medium_area_threshold)]\n",
        "large_objects = train_df[train_df['area_abs'] > medium_area_threshold]\n",
        "\n",
        "print(f\"Small objects (<= {small_area_threshold} px¬≤): {len(small_objects):,} ({len(small_objects)/len(train_df)*100:.1f}%)\")\n",
        "print(f\"Medium objects ({small_area_threshold}-{medium_area_threshold} px¬≤): {len(medium_objects):,} ({len(medium_objects)/len(train_df)*100:.1f}%)\")\n",
        "print(f\"Large objects (> {medium_area_threshold} px¬≤): {len(large_objects):,} ({len(large_objects)/len(train_df)*100:.1f}%)\")\n",
        "\n",
        "if len(small_objects) / len(train_df) > 0.5:\n",
        "    print(\"\\n  Majority of objects are small\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbLqvbykx7kh"
      },
      "source": [
        "## 7. Spatial Distribution Analysis\n",
        "\n",
        "Now we will analyse where objects typically appear in the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb-Cc1AKx7kh"
      },
      "outputs": [],
      "source": [
        "# Spatial distribution analysis\n",
        "print(\" SPATIAL DISTRIBUTION ANALYSIS\")\n",
        "\n",
        "\n",
        "# Create spatial heatmaps\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Object Spatial Distribution', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Overall heatmap\n",
        "x_centers = train_df['x_center_norm'].values\n",
        "y_centers = train_df['y_center_norm'].values\n",
        "\n",
        "# Create 2D histogram for heatmap\n",
        "hist, x_edges, y_edges = np.histogram2d(x_centers, y_centers, bins=20, range=[[0, 1], [0, 1]])\n",
        "extent = [x_edges[0], x_edges[-1], y_edges[0], y_edges[-1]]\n",
        "\n",
        "im1 = axes[0, 0].imshow(hist.T, extent=extent, origin='lower', cmap='YlOrRd', alpha=0.8)\n",
        "axes[0, 0].set_title('Overall Object Distribution Heatmap')\n",
        "axes[0, 0].set_xlabel('X Position (normalized)')\n",
        "axes[0, 0].set_ylabel('Y Position (normalized)')\n",
        "plt.colorbar(im1, ax=axes[0, 0], label='Object Count')\n",
        "\n",
        "# Scatter plot by class\n",
        "colors_scatter = ['red', 'blue', 'green']\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_data = train_df[train_df['class_name'] == class_name]\n",
        "    if len(class_data) > 0:\n",
        "        # Sample data if too many points\n",
        "        if len(class_data) > 1000:\n",
        "            class_data = class_data.sample(1000)\n",
        "\n",
        "        axes[0, 1].scatter(class_data['x_center_norm'], class_data['y_center_norm'],\n",
        "                          alpha=0.6, s=10, label=class_name, c=colors_scatter[i % len(colors_scatter)])\n",
        "\n",
        "axes[0, 1].set_title('Object Positions by Class')\n",
        "axes[0, 1].set_xlabel('X Position (normalized)')\n",
        "axes[0, 1].set_ylabel('Y Position (normalized)')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].set_xlim(0, 1)\n",
        "axes[0, 1].set_ylim(0, 1)\n",
        "\n",
        "# Position distribution histograms\n",
        "axes[1, 0].hist(train_df['x_center_norm'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[1, 0].axvline(0.5, color='red', linestyle='--', label='Center')\n",
        "axes[1, 0].set_title('X Position Distribution')\n",
        "axes[1, 0].set_xlabel('X Position (normalized)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].hist(train_df['y_center_norm'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "axes[1, 1].axvline(0.5, color='red', linestyle='--', label='Center')\n",
        "axes[1, 1].set_title('Y Position Distribution')\n",
        "axes[1, 1].set_xlabel('Y Position (normalized)')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical analysis of positions\n",
        "print(f\"X position statistics:\")\n",
        "print(f\"  Mean: {train_df['x_center_norm'].mean():.3f} (0.5 = center)\")\n",
        "print(f\"  Std: {train_df['x_center_norm'].std():.3f}\")\n",
        "print(f\"  Range: {train_df['x_center_norm'].min():.3f} - {train_df['x_center_norm'].max():.3f}\")\n",
        "\n",
        "print(f\"\\nY position statistics:\")\n",
        "print(f\"  Mean: {train_df['y_center_norm'].mean():.3f} (0.5 = center)\")\n",
        "print(f\"  Std: {train_df['y_center_norm'].std():.3f}\")\n",
        "print(f\"  Range: {train_df['y_center_norm'].min():.3f} - {train_df['y_center_norm'].max():.3f}\")\n",
        "\n",
        "# Edge bias analysis\n",
        "edge_threshold = 0.1  # Objects within 10% of edges\n",
        "edge_objects = train_df[\n",
        "    (train_df['x_center_norm'] <= edge_threshold) |\n",
        "    (train_df['x_center_norm'] >= 1 - edge_threshold) |\n",
        "    (train_df['y_center_norm'] <= edge_threshold) |\n",
        "    (train_df['y_center_norm'] >= 1 - edge_threshold)\n",
        "]\n",
        "\n",
        "edge_percentage = len(edge_objects) / len(train_df) * 100\n",
        "print(f\"\\n Objects near edges (within {edge_threshold*100}%): {len(edge_objects):,} ({edge_percentage:.1f}%)\")\n",
        "\n",
        "if edge_percentage > 30:\n",
        "    print(\" High concentration of objects near edges - consider data augmentation with crops\")\n",
        "elif edge_percentage < 10:\n",
        "    print(\" Objects well distributed across image - good for training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN8v8saQx7kh"
      },
      "source": [
        "## 8. Sample Visualization\n",
        "\n",
        "Now we will visualise some actual images with their annotations to see our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxGrPECNx7kh"
      },
      "outputs": [],
      "source": [
        "def visualize_annotations(images_path, labels_path, class_names, num_samples=6):\n",
        "\n",
        "    # Get random sample of images that have annotations\n",
        "    label_files = [f for f in os.listdir(labels_path) if f.endswith('.txt')]\n",
        "    sample_files = np.random.choice(label_files, min(num_samples, len(label_files)), replace=False)\n",
        "\n",
        "    # Color map for different classes\n",
        "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "    fig.suptitle('Sample Images with Annotations', fontsize=16, fontweight='bold')\n",
        "\n",
        "    for i, label_file in enumerate(sample_files):\n",
        "        if i >= len(axes):\n",
        "            break\n",
        "\n",
        "        # Load image\n",
        "        img_file = label_file.replace('.txt', '.jpg')\n",
        "        img_path = os.path.join(images_path, img_file)\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            img_file = label_file.replace('.txt', '.png')\n",
        "            img_path = os.path.join(images_path, img_file)\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            continue\n",
        "\n",
        "        # Load image with OpenCV\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img_height, img_width = img.shape[:2]\n",
        "\n",
        "        # Load annotations\n",
        "        label_path = os.path.join(labels_path, label_file)\n",
        "        with open(label_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        object_counts = Counter()\n",
        "\n",
        "        # Draw bounding boxes\n",
        "        for line in lines:\n",
        "            if line.strip():\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 5:\n",
        "                    class_id = int(parts[0])\n",
        "                    x_center = float(parts[1]) * img_width\n",
        "                    y_center = float(parts[2]) * img_height\n",
        "                    width = float(parts[3]) * img_width\n",
        "                    height = float(parts[4]) * img_height\n",
        "\n",
        "                    # Convert to corner coordinates\n",
        "                    x1 = int(x_center - width/2)\n",
        "                    y1 = int(y_center - height/2)\n",
        "                    x2 = int(x_center + width/2)\n",
        "                    y2 = int(y_center + height/2)\n",
        "\n",
        "                    # Draw rectangle\n",
        "                    color = colors[class_id % len(colors)]\n",
        "                    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "                    # Add label\n",
        "                    label = class_names[class_id]\n",
        "                    cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "                    object_counts[label] += 1\n",
        "\n",
        "        # Display image\n",
        "        axes[i].imshow(img)\n",
        "\n",
        "        # Create title with object counts\n",
        "        title_parts = []\n",
        "        for class_name in class_names:\n",
        "            count = object_counts.get(class_name, 0)\n",
        "            if count > 0:\n",
        "                title_parts.append(f\"{class_name}: {count}\")\n",
        "\n",
        "        title = f\"{img_file}\\n{', '.join(title_parts)}\"\n",
        "        axes[i].set_title(title, fontsize=10)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    # Hide empty subplots\n",
        "    for j in range(i+1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n SAMPLE IMAGES WITH ANNOTATIONS\")\n",
        "visualize_annotations(train_images_path, train_labels_path, class_names, num_samples=6)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
