{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_title"
      },
      "source": [
        "# YOLOv11 Model Evaluation for Single-Class Vehicle Detection\n",
        "\n",
        "**Research Project**: Comparative Analysis of CNN vs Vision Transformer\n",
        "\n",
        "**Author**: Abdullah Waraich\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, let's check GPU availability and install the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu_check"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q ultralytics supervision scikit-learn seaborn\n",
        "\n",
        "print(\"Libraries installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_section"
      },
      "source": [
        "## 2. Upload Model and Dataset\n",
        "\n",
        "Upload the trained YOLOv11 model (.pt file) and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_files"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "print(\"Upload yolo model (.pt file) and dataset zip file:\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Process uploaded files\n",
        "model_path = None\n",
        "dataset_path = None\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.pt'):\n",
        "        model_path = filename\n",
        "        print(f\"Model found: {filename}\")\n",
        "    elif filename.endswith('.zip'):\n",
        "        # Extract dataset\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('test_dataset')\n",
        "        dataset_path = 'test_dataset'\n",
        "        print(f\"Dataset extracted to: {dataset_path}\")\n",
        "\n",
        "        # Show dataset structure\n",
        "        print(\"\\n Dataset structure:\")\n",
        "        for root, dirs, files in os.walk(dataset_path):\n",
        "            level = root.replace(dataset_path, '').count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            print(f\"{indent}{os.path.basename(root)}/\")\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files[:3]:  # Show first 3 files\n",
        "                print(f\"{subindent}{file}\")\n",
        "            if len(files) > 3:\n",
        "                print(f\"{subindent}... and {len(files)-3} more files\")\n",
        "\n",
        "if not model_path:\n",
        "    print(\" No .pt model file found. Please upload your trained YOLOv11 model.\")\n",
        "if not dataset_path:\n",
        "    print(\" No dataset zip file found. Please upload your test dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_section"
      },
      "source": [
        "## 3. Load Model and Analyze Dataset\n",
        "\n",
        "Now let's load the YOLOv11 model and analyze the test dataset structure for single-class vehicle detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import supervision as sv\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Load YOLOv11 model\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# Get model info\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model classes: {model.names}\")\n",
        "print(f\"Number of classes: {len(model.names)}\")\n",
        "\n",
        "# Define single class for evaluation\n",
        "CLASS_NAMES = ['vehicle']  # Single class\n",
        "print(f\"Target class for evaluation: {CLASS_NAMES[0]}\")\n",
        "\n",
        "# Analyze test dataset\n",
        "images_path = os.path.join(dataset_path, 'images')\n",
        "labels_path = os.path.join(dataset_path, 'labels')\n",
        "\n",
        "# Get list of test images\n",
        "image_files = glob.glob(os.path.join(images_path, '*'))\n",
        "image_files = [f for f in image_files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "print(f\"\\n Found {len(image_files)} total test images\")\n",
        "print(f\" Images directory: {images_path}\")\n",
        "print(f\" Labels directory: {labels_path}\")\n",
        "\n",
        "\n",
        "if len(image_files) == 0:\n",
        "    print(\"No image files found.\")\n",
        "else:\n",
        "    print(\"Dataset loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference_section"
      },
      "source": [
        "## 4. Run Inference with Timing Analysis\n",
        "\n",
        "Let's run inference on all test images and measure the time taken for single-class vehicle detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_inference"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"Processing {len(image_files)} images\")\n",
        "\n",
        "# Storage for results\n",
        "all_predictions = []\n",
        "all_ground_truth = []\n",
        "inference_times = []\n",
        "image_paths = []\n",
        "\n",
        "# Process each image\n",
        "for img_path in tqdm(image_files, desc=\"Running inference\"):\n",
        "    try:\n",
        "        # Load image\n",
        "        image = Image.open(img_path)\n",
        "        image_paths.append(img_path)\n",
        "\n",
        "        # Measure inference time\n",
        "        start_time = time.time()\n",
        "        results = model(img_path, verbose=False)\n",
        "        end_time = time.time()\n",
        "\n",
        "        inference_time = end_time - start_time\n",
        "        inference_times.append(inference_time)\n",
        "\n",
        "        # Extract predictions\n",
        "        detections = results[0].boxes\n",
        "\n",
        "        if detections is not None and len(detections) > 0:\n",
        "            # Convert to supervision format\n",
        "            xyxy = detections.xyxy.cpu().numpy()\n",
        "            confidence = detections.conf.cpu().numpy()\n",
        "            # For single class, all detections are class 0 (vehicle)\n",
        "            class_id = np.zeros(len(xyxy), dtype=int)\n",
        "\n",
        "            pred_detection = sv.Detections(\n",
        "                xyxy=xyxy,\n",
        "                confidence=confidence,\n",
        "                class_id=class_id\n",
        "            )\n",
        "        else:\n",
        "            # No detections\n",
        "            pred_detection = sv.Detections.empty()\n",
        "\n",
        "        all_predictions.append(pred_detection)\n",
        "\n",
        "        # Load ground truth annotations\n",
        "        img_name = Path(img_path).stem\n",
        "        label_path = os.path.join(labels_path, f\"{img_name}.txt\")\n",
        "\n",
        "        if os.path.exists(label_path):\n",
        "            # Getting YOLO format labels\n",
        "            gt_boxes = []\n",
        "            gt_classes = []\n",
        "\n",
        "            with open(label_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            img_width, img_height = image.size\n",
        "\n",
        "            for line in lines:\n",
        "                if line.strip():\n",
        "                    parts = line.strip().split()\n",
        "                    # For single class, class_id is always 0 (vehicle)\n",
        "                    class_id = 0\n",
        "                    x_center = float(parts[1]) * img_width\n",
        "                    y_center = float(parts[2]) * img_height\n",
        "                    width = float(parts[3]) * img_width\n",
        "                    height = float(parts[4]) * img_height\n",
        "\n",
        "                    # Convert to xyxy format\n",
        "                    x1 = x_center - width / 2\n",
        "                    y1 = y_center - height / 2\n",
        "                    x2 = x_center + width / 2\n",
        "                    y2 = y_center + height / 2\n",
        "\n",
        "                    gt_boxes.append([x1, y1, x2, y2])\n",
        "                    gt_classes.append(class_id)\n",
        "\n",
        "            if gt_boxes:\n",
        "                gt_detection = sv.Detections(\n",
        "                    xyxy=np.array(gt_boxes),\n",
        "                    class_id=np.array(gt_classes)\n",
        "                )\n",
        "            else:\n",
        "                gt_detection = sv.Detections.empty()\n",
        "        else:\n",
        "            gt_detection = sv.Detections.empty()\n",
        "\n",
        "        all_ground_truth.append(gt_detection)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {img_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n Inference completed\")\n",
        "print(f\" Average inference time: {np.mean(inference_times):.4f} seconds\")\n",
        "print(f\" Average FPS: {1/np.mean(inference_times):.2f}\")\n",
        "print(f\" Processed {len(all_predictions)} images successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "metrics_section"
      },
      "source": [
        "## 5. Calculate Performance Metrics\n",
        "\n",
        "Calculate comprehensive performance metrics for single-class vehicle detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "calculate_metrics"
      },
      "outputs": [],
      "source": [
        "from supervision.metrics import MeanAveragePrecision\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Calculating Performance Metrics\")\n",
        "\n",
        "# Calculate mAP using supervision\n",
        "map_metric = MeanAveragePrecision()\n",
        "map_result = map_metric.update(all_predictions, all_ground_truth).compute()\n",
        "\n",
        "print(\"MEAN AVERAGE PRECISION (mAP) RESULTS:\")\n",
        "print(f\"mAP@0.5: {map_result.map50:.3f}\")\n",
        "print(f\"mAP@0.5:0.95: {map_result.map50_95:.3f}\")\n",
        "print(f\"mAP@0.75: {map_result.map75:.3f}\")\n",
        "\n",
        "# Calculate precision, recall, F1 Score for \"vehicle\" class\n",
        "print(\"\\n PRECISION, RECALL, F1-SCORE ANALYSIS:\")\n",
        "\n",
        "# Create binary classification arrays for vehicle detection\n",
        "all_pred_binary = []\n",
        "all_true_binary = []\n",
        "\n",
        "for pred, gt in zip(all_predictions, all_ground_truth):\n",
        "    # Binary: has vehicle or not\n",
        "    has_prediction = len(pred) > 0\n",
        "    has_ground_truth = len(gt) > 0\n",
        "\n",
        "    all_pred_binary.append(1 if has_prediction else 0)\n",
        "    all_true_binary.append(1 if has_ground_truth else 0)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_pred_binary = np.array(all_pred_binary)\n",
        "all_true_binary = np.array(all_true_binary)\n",
        "\n",
        "# Calculate binary metrics\n",
        "if np.sum(all_true_binary) > 0:  # Only if we have positive samples\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_true_binary, all_pred_binary, average='binary', zero_division=0\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{CLASS_NAMES[0].upper()} DETECTION:\")\n",
        "    print(f\"  Precision: {precision:.3f}\")\n",
        "    print(f\"  Recall: {recall:.3f}\")\n",
        "    print(f\"  F1-Score: {f1:.3f}\")\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    total_images = len(all_true_binary)\n",
        "    true_positives = np.sum((all_true_binary == 1) & (all_pred_binary == 1))\n",
        "    false_positives = np.sum((all_true_binary == 0) & (all_pred_binary == 1))\n",
        "    false_negatives = np.sum((all_true_binary == 1) & (all_pred_binary == 0))\n",
        "    true_negatives = np.sum((all_true_binary == 0) & (all_pred_binary == 0))\n",
        "\n",
        "    accuracy = (true_positives + true_negatives) / total_images\n",
        "\n",
        "    print(f\"\\n DETAILED METRICS:\")\n",
        "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"  True Positives: {true_positives}\")\n",
        "    print(f\"  False Positives: {false_positives}\")\n",
        "    print(f\"  False Negatives: {false_negatives}\")\n",
        "    print(f\"  True Negatives: {true_negatives}\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\n{CLASS_NAMES[0].upper()}: No ground truth instances found\")\n",
        "    precision = recall = f1 = accuracy = 0.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "confusion_section"
      },
      "source": [
        "## 6. Generate Confusion Matrix\n",
        "\n",
        "Create and visualize the confusion matrix for vehicle detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "confusion_matrix"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Create confusion matrix for binary classification\n",
        "print(\"Generating Confusion Matrix for Vehicle Detection\")\n",
        "\n",
        "# Create binary classes\n",
        "binary_classes = ['No Vehicle', 'Vehicle']\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(all_true_binary, all_pred_binary, labels=[0, 1])\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=binary_classes,\n",
        "            yticklabels=binary_classes)\n",
        "plt.title('YOLOv11 Confusion Matrix\\nSatellite Vehicle Detection', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.ylabel('True', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\n DETAILED CLASSIFICATION REPORT:\")\n",
        "print(classification_report(all_true_binary, all_pred_binary,\n",
        "                          target_names=binary_classes,\n",
        "                          zero_division=0))\n",
        "\n",
        "print(f\"\\n Summary of Statistics:\")\n",
        "print(f\"  Total images: {len(all_true_binary)}\")\n",
        "print(f\"  Images with vehicles (GT): {np.sum(all_true_binary)}\")\n",
        "print(f\"  Images with vehicles (Pred): {np.sum(all_pred_binary)}\")\n",
        "print(f\"  Detection accuracy: {accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timing_section"
      },
      "source": [
        "## 7. Inference Time Analysis\n",
        "\n",
        "Analyze the computational efficiency of the YOLOv11 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timing_analysis"
      },
      "outputs": [],
      "source": [
        "print(\"YOLOV11 INFERENCE TIME ANALYSIS\")\n",
        "\n",
        "# Calculate timing statistics\n",
        "avg_time = np.mean(inference_times)\n",
        "std_time = np.std(inference_times)\n",
        "min_time = np.min(inference_times)\n",
        "max_time = np.max(inference_times)\n",
        "fps = 1 / avg_time\n",
        "\n",
        "print(f\"Timing Statistics:\")\n",
        "print(f\"  Average time per image: {avg_time:.4f} seconds\")\n",
        "print(f\"  Standard deviation: {std_time:.4f} seconds\")\n",
        "print(f\"  Minimum time: {min_time:.4f} seconds\")\n",
        "print(f\"  Maximum time: {max_time:.4f} seconds\")\n",
        "print(f\"  Average FPS: {fps:.2f}\")\n",
        "\n",
        "# Plot inference time distribution\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(inference_times, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.axvline(avg_time, color='red', linestyle='--',\n",
        "           label=f'Mean: {avg_time:.4f}s')\n",
        "plt.xlabel('Inference Time (seconds)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('YOLOv11 Inference Time Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(inference_times[:100], marker='o', markersize=2, alpha=0.7)\n",
        "plt.axhline(avg_time, color='red', linestyle='--',\n",
        "           label=f'Mean: {avg_time:.4f}s')\n",
        "plt.xlabel('Image Index')\n",
        "plt.ylabel('Inference Time (seconds)')\n",
        "plt.title('Inference Time per Image (First 100)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization_section"
      },
      "source": [
        "## 8. Visual Results Analysis\n",
        "\n",
        "Visualize detection results of YOLOv11 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visual_results"
      },
      "outputs": [],
      "source": [
        "print(\"VISUAL RESULTS ANALYSIS\")\n",
        "\n",
        "\n",
        "# Setup visualization components\n",
        "color_palette = sv.ColorPalette.from_hex([\n",
        "    \"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\", \"#96CEB4\", \"#FECA57\", \"#FF9FF3\"\n",
        "])\n",
        "\n",
        "# Select interesting images for visualization\n",
        "num_samples = min(9, len(all_predictions))\n",
        "sample_indices = np.linspace(0, len(all_predictions)-1, num_samples, dtype=int)\n",
        "\n",
        "result_images = []\n",
        "titles = []\n",
        "\n",
        "for idx in sample_indices:\n",
        "    img_path = image_paths[idx]\n",
        "    pred = all_predictions[idx]\n",
        "    gt = all_ground_truth[idx]\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(img_path)\n",
        "\n",
        "    # Calculate text scale based on image size\n",
        "    text_scale = sv.calculate_optimal_text_scale(resolution_wh=image.size)\n",
        "    thickness = sv.calculate_optimal_line_thickness(resolution_wh=image.size)\n",
        "\n",
        "    # Create prediction labels\n",
        "    pred_labels = [\n",
        "        f\"vehicle {confidence:.2f}\"\n",
        "        for confidence in pred.confidence\n",
        "    ] if len(pred) > 0 else []\n",
        "\n",
        "    # Annotate image with predictions\n",
        "    annotated_image = image.copy()\n",
        "\n",
        "    # Draw predictions in color\n",
        "    if len(pred) > 0:\n",
        "        bbox_annotator = sv.BoxAnnotator(color=color_palette, thickness=thickness)\n",
        "        label_annotator = sv.LabelAnnotator(\n",
        "            color=color_palette,\n",
        "            text_color=sv.Color.WHITE,\n",
        "            text_scale=text_scale\n",
        "        )\n",
        "        annotated_image = bbox_annotator.annotate(annotated_image, pred)\n",
        "        annotated_image = label_annotator.annotate(annotated_image, pred, pred_labels)\n",
        "\n",
        "    # Add ground truth boxes in white outline for comparison\n",
        "    if len(gt) > 0:\n",
        "        import cv2\n",
        "        annotated_array = np.array(annotated_image)\n",
        "        for box in gt.xyxy:\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "            cv2.rectangle(annotated_array, (x1, y1), (x2, y2), (255, 255, 255), max(1, thickness//2))\n",
        "        annotated_image = Image.fromarray(annotated_array)\n",
        "\n",
        "    result_images.append(annotated_image)\n",
        "\n",
        "    # Create title with detection count\n",
        "    gt_count = len(gt) if gt is not None else 0\n",
        "    pred_count = len(pred) if pred is not None else 0\n",
        "    titles.append(f\"GT: {gt_count} | Pred: {pred_count}\")\n",
        "\n",
        "# Display results in grid\n",
        "sv.plot_images_grid(\n",
        "    images=result_images,\n",
        "    grid_size=(3, 3),\n",
        "    titles=titles,\n",
        "    size=(15, 15)\n",
        ")\n",
        "\n",
        "print(\"\\n Legend:\")\n",
        "print(\"  - Colored boxes: YOLOv11 predictions with confidence scores\")\n",
        "print(\"  - White outlines: Ground truth annotations\")\n",
        "print(\"  - GT: Ground truth count | Pred: Prediction count\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary_section"
      },
      "source": [
        "## 9. Results Summary and Export\n",
        "\n",
        "Generate a summary of all metrics for research comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results_summary"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"GENERATING RESULTS SUMMARY\")\n",
        "\n",
        "# Create comprehensive results dictionary\n",
        "results_dict = {\n",
        "    'model_info': {\n",
        "        'architecture': 'YOLOv11',\n",
        "        'approach': 'CNN-based',\n",
        "        'classes': CLASS_NAMES,\n",
        "        'num_classes': len(CLASS_NAMES),\n",
        "        'detection_type': 'single-class'\n",
        "    },\n",
        "    'dataset_info': {\n",
        "        'num_test_images': len(image_files),\n",
        "        'image_resolution': '640x640',\n",
        "        'processed_successfully': len(all_predictions)\n",
        "    },\n",
        "    'performance_metrics': {\n",
        "        'mAP': {\n",
        "            'mAP50': float(map_result.map50),\n",
        "            'mAP50_95': float(map_result.map50_95),\n",
        "            'mAP75': float(map_result.map75)\n",
        "        },\n",
        "        'binary_classification': {\n",
        "            'precision': float(precision),\n",
        "            'recall': float(recall),\n",
        "            'f1_score': float(f1),\n",
        "            'accuracy': float(accuracy)\n",
        "        },\n",
        "        'detection_counts': {\n",
        "            'true_positives': int(true_positives),\n",
        "            'false_positives': int(false_positives),\n",
        "            'false_negatives': int(false_negatives),\n",
        "            'true_negatives': int(true_negatives)\n",
        "        }\n",
        "    },\n",
        "    'computational_efficiency': {\n",
        "        'avg_inference_time': float(avg_time),\n",
        "        'std_inference_time': float(std_time),\n",
        "        'min_inference_time': float(min_time),\n",
        "        'max_inference_time': float(max_time),\n",
        "        'avg_fps': float(fps)\n",
        "    },\n",
        "    'evaluation_timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "# Display formatted summary\n",
        "print(\"YOLOv11 EVALUATION SUMMARY\")\n",
        "print(f\"\\n Dataset: {len(image_files)} test images\")\n",
        "print(f\" Target class: {CLASS_NAMES[0]}\")\n",
        "print(\"\\nKEY PERFORMANCE METRICS:\")\n",
        "print(f\"  - mAP@0.5: {map_result.map50:.3f}\")\n",
        "print(f\"  - mAP@0.5:0.95: {map_result.map50_95:.3f}\")\n",
        "print(f\"  - Precision: {precision:.3f}\")\n",
        "print(f\"  - Recall: {recall:.3f}\")\n",
        "print(f\"  - F1-Score: {f1:.3f}\")\n",
        "print(f\"  - Accuracy: {accuracy:.3f}\")\n",
        "print(\"\\n COMPUTATIONAL EFFICIENCY:\")\n",
        "print(f\"  - Average inference time: {avg_time:.4f} seconds\")\n",
        "print(f\"  - Average FPS: {fps:.2f}\")\n",
        "print(f\"  - Processing speed: {len(image_files)/sum(inference_times):.2f} images/second\")\n",
        "\n",
        "print(\"\\n DETECTION PERFORMANCE:\")\n",
        "print(f\"  - True Positives: {true_positives}\")\n",
        "print(f\"  - False Positives: {false_positives}\")\n",
        "print(f\"  - False Negatives: {false_negatives}\")\n",
        "print(f\"  - True Negatives: {true_negatives}\")\n",
        "\n",
        "# Save results to JSON file\n",
        "with open('yolov11_single_class_results.json', 'w') as f:\n",
        "    json.dump(results_dict, f, indent=2)\n",
        "\n",
        "# Download results file\n",
        "try:\n",
        "    files.download('yolov11_single_class_results.json')\n",
        "\n",
        "except:\n",
        "    print(\"Results file saved locally (download from file panel)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison_section"
      },
      "source": [
        "## 10. Prepare Data for Research Comparison\n",
        "\n",
        "Generate data in format suitable for McNemar's test and statistical comparison with RF-DETR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "comparison_data"
      },
      "outputs": [],
      "source": [
        "print(\"PREPARING DATA FOR STATISTICAL COMPARISON\")\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"Calculate Intersection over Union of two bounding boxes\"\"\"\n",
        "    x1_min, y1_min, x1_max, y1_max = box1\n",
        "    x2_min, y2_min, x2_max, y2_max = box2\n",
        "\n",
        "    # Calculate intersection\n",
        "    inter_x_min = max(x1_min, x2_min)\n",
        "    inter_y_min = max(y1_min, y2_min)\n",
        "    inter_x_max = min(x1_max, x2_max)\n",
        "    inter_y_max = min(y1_max, y2_max)\n",
        "\n",
        "    if inter_x_max <= inter_x_min or inter_y_max <= inter_y_min:\n",
        "        return 0.0\n",
        "\n",
        "    inter_area = (inter_x_max - inter_x_min) * (inter_y_max - inter_y_min)\n",
        "\n",
        "    # Calculate union\n",
        "    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
        "    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area > 0 else 0.0\n",
        "\n",
        "# Create per-image detection results for McNemar's test\n",
        "detection_results = []\n",
        "\n",
        "for i, (pred, gt, img_path) in enumerate(zip(all_predictions, all_ground_truth, image_paths)):\n",
        "    img_name = Path(img_path).stem\n",
        "\n",
        "    # Calculate if detection was successful (IoU-based)\n",
        "    has_ground_truth = len(gt) > 0\n",
        "    has_prediction = len(pred) > 0\n",
        "\n",
        "    # Check if any prediction matches ground truth (IoU > 0.5)\n",
        "    detection_success = False\n",
        "    if has_ground_truth and has_prediction:\n",
        "        for gt_box in gt.xyxy:\n",
        "            for pred_box, conf in zip(pred.xyxy, pred.confidence):\n",
        "                if conf > 0.5:\n",
        "                    iou = calculate_iou(gt_box, pred_box)\n",
        "                    if iou > 0.5:\n",
        "                        detection_success = True\n",
        "                        break\n",
        "            if detection_success:\n",
        "                break\n",
        "    elif not has_ground_truth and not has_prediction:\n",
        "        detection_success = True  # Correct negative\n",
        "\n",
        "    detection_results.append({\n",
        "        'image_name': img_name,\n",
        "        'has_ground_truth': has_ground_truth,\n",
        "        'has_prediction': has_prediction,\n",
        "        'detection_success': detection_success,\n",
        "        'num_gt_objects': len(gt),\n",
        "        'num_pred_objects': len(pred),\n",
        "        'inference_time': inference_times[i]\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame for easy analysis\n",
        "results_df = pd.DataFrame(detection_results)\n",
        "\n",
        "# Calculate success rate\n",
        "success_rate = results_df['detection_success'].mean()\n",
        "print(f\"🎯 Overall Detection Success Rate: {success_rate:.3f}\")\n",
        "\n",
        "# Save detection results for McNemar's test\n",
        "results_df.to_csv('yolov11_single_class_detection_results.csv', index=False)\n",
        "\n",
        "print(f\"\\n Detection Results Summary:\")\n",
        "print(f\"  - Images with ground truth: {results_df['has_ground_truth'].sum()}\")\n",
        "print(f\"  - Images with predictions: {results_df['has_prediction'].sum()}\")\n",
        "print(f\"  - Successful detections: {results_df['detection_success'].sum()}\")\n",
        "print(f\"  - Success rate: {success_rate:.1%}\")\n",
        "\n",
        "# Generate research comparison summary\n",
        "comparison_summary = f\"\"\"\n",
        "YOLOv11 vs RF-DETR SINGLE-CLASS COMPARISON SUMMARY\n",
        "==================================================\n",
        "\n",
        "Research Project: CNN vs Vision Transformer for Satellite Vehicle Detection\n",
        "Student: Abdullah Waraich (ID: 2401554)\n",
        "Supervisor: Dr. Adrian Clark\n",
        "\n",
        "ARCHITECTURE COMPARISON:\n",
        "• YOLOv11: CNN-based single-stage detector\n",
        "• RF-DETR: Vision Transformer-based detector\n",
        "\n",
        "MODEL CONFIGURATION:\n",
        "• Single class: {CLASS_NAMES[0]}\n",
        "• Simplified evaluation pipeline\n",
        "• Clean comparison without class imbalance issues\n",
        "\n",
        "KEY FINDINGS:\n",
        "• YOLOv11 mAP@0.5: {map_result.map50:.3f}\n",
        "• RF-DETR mAP@0.5: [To be filled from RF-DETR evaluation]\n",
        "• YOLOv11 inference time: {avg_time:.4f}s\n",
        "• RF-DETR inference time: [To be filled from RF-DETR evaluation]\n",
        "• YOLOv11 accuracy: {accuracy:.3f}\n",
        "• RF-DETR accuracy: [To be filled from RF-DETR evaluation]\n",
        "\n",
        "NEXT STEPS:\n",
        "1. Run McNemar's Test for statistical significance\n",
        "2. Compare computational efficiency\n",
        "3. Analyze failure cases and strengths\n",
        "4. Complete dissertation analysis\n",
        "\n",
        "Files generated:\n",
        "• yolov11_single_class_results.json\n",
        "• yolov11_single_class_detection_results.csv\n",
        "\"\"\"\n",
        "\n",
        "# Save comparison summary\n",
        "with open('yolov11_research_comparison.txt', 'w') as f:\n",
        "    f.write(comparison_summary)\n",
        "\n",
        "print(comparison_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcnemar_section"
      },
      "source": [
        "## 11. McNemar's Test Preparation\n",
        "\n",
        "Prepare the data structure needed for McNemar's statistical test when comparing with RF-DETR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcnemar_prep"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade scipy\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "print(\"McNEMAR'S TEST PREPARATION\")\n",
        "\n",
        "\n",
        "# Create binary success/failure array for this model\n",
        "yolo_results = results_df['detection_success'].values.astype(int)\n",
        "\n",
        "print(f\"\\n YOLOv11 Detection Results Summary:\")\n",
        "print(f\"  - Total images: {len(yolo_results)}\")\n",
        "print(f\"  - Successful detections: {np.sum(yolo_results)}\")\n",
        "print(f\"  - Failed detections: {len(yolo_results) - np.sum(yolo_results)}\")\n",
        "print(f\"  - Success rate: {np.mean(yolo_results):.3f}\")\n",
        "\n",
        "# Save binary results for McNemar's test\n",
        "mcnemar_data = {\n",
        "    'image_names': results_df['image_name'].tolist(),\n",
        "    'yolov11_success': yolo_results.tolist(),\n",
        "    'yolov11_metrics': {\n",
        "        'mAP50': float(map_result.map50),\n",
        "        'success_rate': float(np.mean(yolo_results)),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1_score': float(f1),\n",
        "        'accuracy': float(accuracy)\n",
        "    },\n",
        "    'model_info': {\n",
        "        'architecture': 'YOLOv11',\n",
        "        'type': 'CNN-based',\n",
        "        'classes': CLASS_NAMES\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('mcnemar_yolov11_single_class.json', 'w') as f:\n",
        "    json.dump(mcnemar_data, f, indent=2)\n",
        "\n",
        "print(\"\\n McNemar's test data saved to: mcnemar_yolov11_single_class.json\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}